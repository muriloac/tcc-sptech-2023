{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general imports\n",
    "from shutil import rmtree\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data science imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cxr_dataset as CXR\n",
    "import eval_model as E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Números de GPUs ativas:1\n"
     ]
    }
   ],
   "source": [
    "use_gpu = torch.cuda.is_available()\n",
    "gpu_count = torch.cuda.device_count()\n",
    "print(\"Números de GPUs ativas:\" + str(gpu_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkpoint(model, best_loss, epoch, LR):\n",
    "    \"\"\"\n",
    "    Salvando o checkpoint do modelo\n",
    "\n",
    "    Args:\n",
    "        model: modelo a ser salvo\n",
    "        best_loss: melhor loss obtido até o momento\n",
    "        epoch: número da epoch atual\n",
    "        LR: learning rate atual\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    print('Salvando checkpoint...')\n",
    "    state = {\n",
    "        'model': model,\n",
    "        'best_loss': best_loss,\n",
    "        'epoch': epoch,\n",
    "        'rng_state': torch.get_rng_state(),\n",
    "        'LR': LR\n",
    "    }\n",
    "\n",
    "    torch.save(state, 'results/checkpoint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train_model(\n",
    "        model,\n",
    "        criterion,\n",
    "        optimizer,\n",
    "        LR,\n",
    "        num_epochs,\n",
    "        dataloaders,\n",
    "        dataset_sizes,\n",
    "        weight_decay):\n",
    "    \"\"\"\n",
    "    Ajusta um modelo torchvision para dados CXR da NIH.\n",
    "\n",
    "    Args:\n",
    "        model: modelo torchvision a ser ajustado (densenet-121 neste caso)\n",
    "        criterion: critério de perda (perda de entropia cruzada binária, BCELoss)\n",
    "        optimizer: otimizador a ser usado no treinamento (SGD)\n",
    "        LR: taxa de aprendizado\n",
    "        num_epochs: continuar o treinamento até este número de epochs\n",
    "        dataloaders: dataloaders de treinamento e validação do PyTorch\n",
    "        dataset_sizes: comprimento dos datasets de treinamento e validação\n",
    "        weight_decay: parâmetro de decaimento de peso que usamos no SGD com momentum\n",
    "    Returns:\n",
    "        model: modelo torchvision treinado\n",
    "        best_epoch: epoch em que a melhor perda de validação do modelo foi obtida\n",
    "\n",
    "    \"\"\"\n",
    "    since = time.time()\n",
    "\n",
    "    start_epoch = 1\n",
    "    best_loss = 999999\n",
    "    best_epoch = -1\n",
    "    last_train_loss = -1\n",
    "\n",
    "    # iterar sobre as epochs\n",
    "    for epoch in range(start_epoch, num_epochs + 1):\n",
    "        print('epoch {}/{}'.format(epoch, num_epochs))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # definir o modelo para o modo de treinamento ou avaliação com base em\n",
    "        # se estamos no treinamento ou na validação; necessário para obter previsões corretas dada a batchnorm\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train(True)\n",
    "            else:\n",
    "                model.train(False)\n",
    "\n",
    "            running_loss = 0.0\n",
    "\n",
    "            i = 0\n",
    "            total_done = 0\n",
    "            # iterar sobre todos os dados no dataloader de treinamento/validação:\n",
    "            for data in tqdm(dataloaders[phase]):\n",
    "                i += 1\n",
    "                inputs, labels, _ = data\n",
    "                batch_size = inputs.shape[0]\n",
    "                inputs = Variable(inputs.cuda())\n",
    "                labels = Variable(labels.cuda()).float()\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                # calcular o gradiente e atualizar os parâmetros na fase de treinamento\n",
    "                optimizer.zero_grad()\n",
    "                loss = criterion(outputs, labels)\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                running_loss += loss.data * batch_size\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "\n",
    "            if phase == 'train':\n",
    "                last_train_loss = epoch_loss\n",
    "\n",
    "            print(phase + ' epoch {}:loss {:.4f} with data size {}'.format(\n",
    "                epoch, epoch_loss, dataset_sizes[phase]))\n",
    "\n",
    "            # diminuir a taxa de aprendizado se não houver melhoria na perda de validação nesta epoch\n",
    "            if phase == 'val' and epoch_loss > best_loss:\n",
    "                print(\"diminuindo a taxa de aprendizado de \" + str(LR) + \" para \" +\n",
    "                      str(LR / 10) + \" pois não estamos vendo melhoria na perda de validação\")\n",
    "                LR = LR / 10\n",
    "                # criar um novo otimizador com uma taxa de aprendizado menor\n",
    "                optimizer = optim.SGD(\n",
    "                    filter(\n",
    "                        lambda p: p.requires_grad,\n",
    "                        model.parameters()),\n",
    "                    lr=LR,\n",
    "                    momentum=0.9,\n",
    "                    weight_decay=weight_decay)\n",
    "                print(\"criado novo otimizador com taxa de aprendizado \" + str(LR))\n",
    "\n",
    "            # salvar um checkpoint do modelo se tiver a melhor perda de validação até agora\n",
    "            if phase == 'val' and epoch_loss < best_loss:\n",
    "                best_loss = epoch_loss\n",
    "                best_epoch = epoch\n",
    "                checkpoint(model, best_loss, epoch, LR)\n",
    "\n",
    "            # registrar a perda de treinamento e validação em cada epoch\n",
    "            if phase == 'val':\n",
    "                with open(\"results/log_train\", 'a') as logfile:\n",
    "                    logwriter = csv.writer(logfile, delimiter=',')\n",
    "                    if(epoch == 1):\n",
    "                        logwriter.writerow([\"epoch\", \"train_loss\", \"val_loss\"])\n",
    "                    logwriter.writerow([epoch, last_train_loss, epoch_loss])\n",
    "\n",
    "        total_done += batch_size\n",
    "        if(total_done % (100 * batch_size) == 0):\n",
    "            print(\"completado \" + str(total_done) + \" até agora na epoch\")\n",
    "\n",
    "        # interromper se não houver melhoria na perda de validação em 3 epochs\n",
    "        if ((epoch - best_epoch) >= 3):\n",
    "            print(\"sem melhoria em 3 epochs, interrompendo\")\n",
    "            break\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Treinamento completo em {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "    # carregar os melhores pesos do modelo para retornar\n",
    "    checkpoint_best = torch.load('results/checkpoint')\n",
    "    model = checkpoint_best['model']\n",
    "\n",
    "    return model, best_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cnn(PATH_TO_IMAGES, LR, WEIGHT_DECAY):\n",
    "    \"\"\"\n",
    "    Treina um modelo torchvision com dados da NIH, dados hiperparâmetros de alto nível.\n",
    "\n",
    "    Args:\n",
    "        PATH_TO_IMAGES: caminho para as imagens da NIH\n",
    "        LR: taxa de aprendizado\n",
    "        WEIGHT_DECAY: parâmetro de decaimento de peso para SGD\n",
    "\n",
    "    Returns:\n",
    "        preds: previsões do modelo torchvision no conjunto de teste com a verdadeira para comparação\n",
    "        aucs: AUCs para cada par de treino e teste\n",
    "\n",
    "    \"\"\"\n",
    "    NUM_EPOCHS = 20\n",
    "    BATCH_SIZE = 14\n",
    "\n",
    "    try:\n",
    "        rmtree('results/')\n",
    "    except BaseException:\n",
    "        pass  # o diretório ainda não existe, não há necessidade de limpá-lo\n",
    "    os.makedirs(\"results/\")\n",
    "\n",
    "    # use a média e o desvio padrão do ImageNet para normalização\n",
    "    mean = [0.485, 0.456, 0.406]\n",
    "    std = [0.229, 0.224, 0.225]\n",
    "\n",
    "    N_LABELS = 14  # estamos prevendo 14 rótulos\n",
    "\n",
    "    # defina as transformações torchvision\n",
    "    data_transforms = {\n",
    "        'train': transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.Resize(224),\n",
    "            # porque o redimensionamento nem sempre dá 224 x 224, isso garante 224 x 224\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean, std)\n",
    "        ]),\n",
    "        'val': transforms.Compose([\n",
    "            transforms.Resize(224),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean, std)\n",
    "        ]),\n",
    "    }\n",
    "\n",
    "    # crie dataloaders de treino/val\n",
    "    transformed_datasets = {}\n",
    "    transformed_datasets['train'] = CXR.CXRDataset(\n",
    "        path_to_images=PATH_TO_IMAGES,\n",
    "        fold='train',\n",
    "        transform=data_transforms['train'])\n",
    "    transformed_datasets['val'] = CXR.CXRDataset(\n",
    "        path_to_images=PATH_TO_IMAGES,\n",
    "        fold='val',\n",
    "        transform=data_transforms['val'])\n",
    "\n",
    "    dataloaders = {}\n",
    "    dataloaders['train'] = torch.utils.data.DataLoader(\n",
    "        transformed_datasets['train'],\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=8)\n",
    "    dataloaders['val'] = torch.utils.data.DataLoader(\n",
    "        transformed_datasets['val'],\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=8)\n",
    "    \n",
    "    \n",
    "    if not use_gpu:\n",
    "        raise ValueError(\"Erro, requer GPU\")\n",
    "    \n",
    "    model = models.densenet121(weights='DEFAULT')\n",
    "    num_ftrs = model.classifier.in_features\n",
    "    # adicione a camada final com # de saídas na mesma dimensão dos rótulos com ativação sigmoidal\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Linear(num_ftrs, N_LABELS), nn.Sigmoid())\n",
    "\n",
    "    # coloque o modelo na GPU\n",
    "    model = model.cuda()\n",
    "\n",
    "    # defina o critério, otimizador para treinamento\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.SGD(\n",
    "        filter(\n",
    "            lambda p: p.requires_grad,\n",
    "            model.parameters()),\n",
    "        lr=LR,\n",
    "        momentum=0.9,\n",
    "        weight_decay=WEIGHT_DECAY)\n",
    "    dataset_sizes = {x: len(transformed_datasets[x]) for x in ['train', 'val']}\n",
    "\n",
    "    # treine o modelo\n",
    "    model, best_epoch = train_model(model, criterion, optimizer, LR, num_epochs=NUM_EPOCHS,\n",
    "                                    dataloaders=dataloaders, dataset_sizes=dataset_sizes, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "    # obtenha previsões e AUCs no conjunto de teste\n",
    "    preds, aucs = E.make_pred_multilabel(\n",
    "        data_transforms, model, PATH_TO_IMAGES)\n",
    "\n",
    "    return preds, aucs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/20\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5605/5605 [08:51<00:00, 10.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch 1:loss 0.1623 with data size 78468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 802/802 [00:47<00:00, 16.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val epoch 1:loss 0.1530 with data size 11219\n",
      "Salvando checkpoint...\n",
      "epoch 2/20\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5605/5605 [08:52<00:00, 10.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch 2:loss 0.1513 with data size 78468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 802/802 [00:43<00:00, 18.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val epoch 2:loss 0.1528 with data size 11219\n",
      "Salvando checkpoint...\n",
      "epoch 3/20\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5605/5605 [08:52<00:00, 10.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch 3:loss 0.1469 with data size 78468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 802/802 [00:43<00:00, 18.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val epoch 3:loss 0.1489 with data size 11219\n",
      "Salvando checkpoint...\n",
      "epoch 4/20\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5605/5605 [08:46<00:00, 10.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch 4:loss 0.1439 with data size 78468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 802/802 [00:43<00:00, 18.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val epoch 4:loss 0.1485 with data size 11219\n",
      "Salvando checkpoint...\n",
      "epoch 5/20\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5605/5605 [08:48<00:00, 10.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch 5:loss 0.1413 with data size 78468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 802/802 [00:43<00:00, 18.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val epoch 5:loss 0.1485 with data size 11219\n",
      "diminuindo a taxa de aprendizado de 0.01 para 0.001 pois não estamos vendo melhoria na perda de validação\n",
      "criado novo otimizador com taxa de aprendizado 0.001\n",
      "epoch 6/20\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5605/5605 [08:49<00:00, 10.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch 6:loss 0.1312 with data size 78468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 802/802 [00:42<00:00, 18.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val epoch 6:loss 0.1457 with data size 11219\n",
      "Salvando checkpoint...\n",
      "epoch 7/20\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5605/5605 [08:37<00:00, 10.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch 7:loss 0.1272 with data size 78468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 802/802 [00:43<00:00, 18.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val epoch 7:loss 0.1472 with data size 11219\n",
      "diminuindo a taxa de aprendizado de 0.001 para 0.0001 pois não estamos vendo melhoria na perda de validação\n",
      "criado novo otimizador com taxa de aprendizado 0.0001\n",
      "epoch 8/20\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5605/5605 [08:39<00:00, 10.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch 8:loss 0.1238 with data size 78468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 802/802 [00:43<00:00, 18.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val epoch 8:loss 0.1469 with data size 11219\n",
      "diminuindo a taxa de aprendizado de 0.0001 para 1e-05 pois não estamos vendo melhoria na perda de validação\n",
      "criado novo otimizador com taxa de aprendizado 1e-05\n",
      "epoch 9/20\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5605/5605 [08:48<00:00, 10.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch 9:loss 0.1233 with data size 78468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 802/802 [00:45<00:00, 17.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val epoch 9:loss 0.1468 with data size 11219\n",
      "diminuindo a taxa de aprendizado de 1e-05 para 1.0000000000000002e-06 pois não estamos vendo melhoria na perda de validação\n",
      "criado novo otimizador com taxa de aprendizado 1.0000000000000002e-06\n",
      "epoch 10/20\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5605/5605 [08:49<00:00, 10.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch 10:loss 0.1232 with data size 78468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 802/802 [00:43<00:00, 18.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val epoch 10:loss 0.1471 with data size 11219\n",
      "diminuindo a taxa de aprendizado de 1.0000000000000002e-06 para 1.0000000000000002e-07 pois não estamos vendo melhoria na perda de validação\n",
      "criado novo otimizador com taxa de aprendizado 1.0000000000000002e-07\n",
      "epoch 11/20\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5605/5605 [08:45<00:00, 10.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch 11:loss 0.1232 with data size 78468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 802/802 [00:43<00:00, 18.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val epoch 11:loss 0.1469 with data size 11219\n",
      "diminuindo a taxa de aprendizado de 1.0000000000000002e-07 para 1.0000000000000002e-08 pois não estamos vendo melhoria na perda de validação\n",
      "criado novo otimizador com taxa de aprendizado 1.0000000000000002e-08\n",
      "sem melhoria em 3 epochs, interrompendo\n",
      "Treinamento completo em 104m 44s\n",
      "0\n",
      "160\n",
      "320\n",
      "480\n",
      "640\n",
      "800\n",
      "960\n",
      "1120\n",
      "1280\n",
      "1440\n",
      "1600\n",
      "1760\n",
      "1920\n",
      "2080\n",
      "2240\n",
      "2400\n",
      "2560\n",
      "2720\n",
      "2880\n",
      "3040\n",
      "3200\n",
      "3360\n",
      "3520\n",
      "3680\n",
      "3840\n",
      "4000\n",
      "4160\n",
      "4320\n",
      "4480\n",
      "4640\n",
      "4800\n",
      "4960\n",
      "5120\n",
      "5280\n",
      "5440\n",
      "5600\n",
      "5760\n",
      "5920\n",
      "6080\n",
      "6240\n",
      "6400\n",
      "6560\n",
      "6720\n",
      "6880\n",
      "7040\n",
      "7200\n",
      "7360\n",
      "7520\n",
      "7680\n",
      "7840\n",
      "8000\n",
      "8160\n",
      "8320\n",
      "8480\n",
      "8640\n",
      "8800\n",
      "8960\n",
      "9120\n",
      "9280\n",
      "9440\n",
      "9600\n",
      "9760\n",
      "9920\n",
      "10080\n",
      "10240\n",
      "10400\n",
      "10560\n",
      "10720\n",
      "10880\n",
      "11040\n",
      "11200\n",
      "11360\n",
      "11520\n",
      "11680\n",
      "11840\n",
      "12000\n",
      "12160\n",
      "12320\n",
      "12480\n",
      "12640\n",
      "12800\n",
      "12960\n",
      "13120\n",
      "13280\n",
      "13440\n",
      "13600\n",
      "13760\n",
      "13920\n",
      "14080\n",
      "14240\n",
      "14400\n",
      "14560\n",
      "14720\n",
      "14880\n",
      "15040\n",
      "15200\n",
      "15360\n",
      "15520\n",
      "15680\n",
      "15840\n",
      "16000\n",
      "16160\n",
      "16320\n",
      "16480\n",
      "16640\n",
      "16800\n",
      "16960\n",
      "17120\n",
      "17280\n",
      "17440\n",
      "17600\n",
      "17760\n",
      "17920\n",
      "18080\n",
      "18240\n",
      "18400\n",
      "18560\n",
      "18720\n",
      "18880\n",
      "19040\n",
      "19200\n",
      "19360\n",
      "19520\n",
      "19680\n",
      "19840\n",
      "20000\n",
      "20160\n",
      "20320\n",
      "20480\n",
      "20640\n",
      "20800\n",
      "20960\n",
      "21120\n",
      "21280\n",
      "21440\n",
      "21600\n",
      "21760\n",
      "21920\n",
      "22080\n",
      "22240\n",
      "22400\n"
     ]
    }
   ],
   "source": [
    "PATH_TO_IMAGES = \"./images/\"\n",
    "WEIGHT_DECAY = 1e-4\n",
    "LEARNING_RATE = 0.01\n",
    "preds, aucs = train_cnn(PATH_TO_IMAGES, LEARNING_RATE, WEIGHT_DECAY)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
