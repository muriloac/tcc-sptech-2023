{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-13T22:30:30.861878900Z",
     "start_time": "2023-09-13T22:30:28.999227400Z"
    }
   },
   "outputs": [],
   "source": [
    "# pytorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-13T22:30:30.869145900Z",
     "start_time": "2023-09-13T22:30:30.863634900Z"
    }
   },
   "outputs": [],
   "source": [
    "# general imports\n",
    "from shutil import rmtree\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-13T22:30:30.876657200Z",
     "start_time": "2023-09-13T22:30:30.866145Z"
    }
   },
   "outputs": [],
   "source": [
    "# data science imports\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-13T22:30:31.388559600Z",
     "start_time": "2023-09-13T22:30:30.873651800Z"
    }
   },
   "outputs": [],
   "source": [
    "import cxr_dataset as CXR\n",
    "import eval_model as E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-13T22:30:31.415913500Z",
     "start_time": "2023-09-13T22:30:31.390560300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Números de GPUs ativas:1\n"
     ]
    }
   ],
   "source": [
    "use_gpu = torch.cuda.is_available()\n",
    "gpu_count = torch.cuda.device_count()\n",
    "print(\"Números de GPUs ativas:\" + str(gpu_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-13T22:30:31.416915200Z",
     "start_time": "2023-09-13T22:30:31.410979400Z"
    }
   },
   "outputs": [],
   "source": [
    "def checkpoint(model, best_loss, epoch, LR):\n",
    "    \"\"\"\n",
    "    Salvando o checkpoint do modelo\n",
    "\n",
    "    Args:\n",
    "        model: modelo a ser salvo\n",
    "        best_loss: melhor loss obtido até o momento\n",
    "        epoch: número da epoch atual\n",
    "        LR: learning rate atual\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    print('Salvando checkpoint...')\n",
    "    state = {\n",
    "        'model': model,\n",
    "        'best_loss': best_loss,\n",
    "        'epoch': epoch,\n",
    "        'rng_state': torch.get_rng_state(),\n",
    "        'LR': LR\n",
    "    }\n",
    "\n",
    "    torch.save(state, 'results/checkpoint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-13T22:30:31.452249300Z",
     "start_time": "2023-09-13T22:30:31.420917800Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train_model(\n",
    "        model,\n",
    "        criterion,\n",
    "        optimizer,\n",
    "        LR,\n",
    "        num_epochs,\n",
    "        dataloaders,\n",
    "        dataset_sizes,\n",
    "        weight_decay):\n",
    "    \"\"\"\n",
    "    Ajusta um modelo torchvision para dados CXR da NIH.\n",
    "\n",
    "    Args:\n",
    "        model: modelo torchvision a ser ajustado (densenet-121 neste caso)\n",
    "        criterion: critério de perda (perda de entropia cruzada binária, BCELoss)\n",
    "        optimizer: otimizador a ser usado no treinamento (SGD)\n",
    "        LR: taxa de aprendizado\n",
    "        num_epochs: continuar o treinamento até este número de epochs\n",
    "        dataloaders: dataloaders de treinamento e validação do PyTorch\n",
    "        dataset_sizes: comprimento dos datasets de treinamento e validação\n",
    "        weight_decay: parâmetro de decaimento de peso que usamos no SGD com momentum\n",
    "    Returns:\n",
    "        model: modelo torchvision treinado\n",
    "        best_epoch: epoch em que a melhor perda de validação do modelo foi obtida\n",
    "\n",
    "    \"\"\"\n",
    "    since = time.time()\n",
    "\n",
    "    start_epoch = 1\n",
    "    best_loss = 999999\n",
    "    best_epoch = -1\n",
    "    last_train_loss = -1\n",
    "\n",
    "    # iterar sobre as epochs\n",
    "    for epoch in range(start_epoch, num_epochs + 1):\n",
    "        print('epoch {}/{}'.format(epoch, num_epochs))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # definir o modelo para o modo de treinamento ou avaliação com base em\n",
    "        # se estamos no treinamento ou na validação; necessário para obter previsões corretas dada a batchnorm\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train(True)\n",
    "            else:\n",
    "                model.train(False)\n",
    "\n",
    "            running_loss = 0.0\n",
    "\n",
    "            i = 0\n",
    "            total_done = 0\n",
    "            # iterar sobre todos os dados no dataloader de treinamento/validação:\n",
    "            for data in tqdm(dataloaders[phase]):\n",
    "                i += 1\n",
    "                inputs, labels, _ = data\n",
    "                batch_size = inputs.shape[0]\n",
    "                inputs = Variable(inputs.cuda())\n",
    "                labels = Variable(labels.cuda()).float()\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                # calcular o gradiente e atualizar os parâmetros na fase de treinamento\n",
    "                optimizer.zero_grad()\n",
    "                loss = criterion(outputs, labels)\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                running_loss += loss.data * batch_size\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "\n",
    "            if phase == 'train':\n",
    "                last_train_loss = epoch_loss\n",
    "\n",
    "            print(phase + ' epoch {}:loss {:.4f} with data size {}'.format(\n",
    "                epoch, epoch_loss, dataset_sizes[phase]))\n",
    "\n",
    "            # diminuir a taxa de aprendizado se não houver melhoria na perda de validação nesta epoch\n",
    "            if phase == 'val' and epoch_loss > best_loss:\n",
    "                print(\"diminuindo a taxa de aprendizado de \" + str(LR) + \" para \" +\n",
    "                      str(LR / 10) + \" pois não estamos vendo melhoria na perda de validação\")\n",
    "                LR = LR / 10\n",
    "                # criar um novo otimizador com uma taxa de aprendizado menor\n",
    "                optimizer = optim.SGD(\n",
    "                    filter(\n",
    "                        lambda p: p.requires_grad,\n",
    "                        model.parameters()),\n",
    "                    lr=LR,\n",
    "                    momentum=0.9,\n",
    "                    weight_decay=weight_decay)\n",
    "                print(\"criado novo otimizador com taxa de aprendizado \" + str(LR))\n",
    "\n",
    "            # salvar um checkpoint do modelo se tiver a melhor perda de validação até agora\n",
    "            if phase == 'val' and epoch_loss < best_loss:\n",
    "                best_loss = epoch_loss\n",
    "                best_epoch = epoch\n",
    "                checkpoint(model, best_loss, epoch, LR)\n",
    "\n",
    "            # registrar a perda de treinamento e validação em cada epoch\n",
    "            if phase == 'val':\n",
    "                with open(\"results/log_train\", 'a') as logfile:\n",
    "                    logwriter = csv.writer(logfile, delimiter=',')\n",
    "                    if(epoch == 1):\n",
    "                        logwriter.writerow([\"epoch\", \"train_loss\", \"val_loss\"])\n",
    "                    logwriter.writerow([epoch, last_train_loss, epoch_loss])\n",
    "\n",
    "        total_done += batch_size\n",
    "        if(total_done % (100 * batch_size) == 0):\n",
    "            print(\"completado \" + str(total_done) + \" até agora na epoch\")\n",
    "\n",
    "        # interromper se não houver melhoria na perda de validação em 3 epochs\n",
    "        if ((epoch - best_epoch) >= 3):\n",
    "            print(\"sem melhoria em 3 epochs, interrompendo\")\n",
    "            break\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Treinamento completo em {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "    # carregar os melhores pesos do modelo para retornar\n",
    "    checkpoint_best = torch.load('results/checkpoint')\n",
    "    model = checkpoint_best['model']\n",
    "\n",
    "    return model, best_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-13T22:30:31.460262Z",
     "start_time": "2023-09-13T22:30:31.424850700Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_cnn(PATH_TO_IMAGES, LR, WEIGHT_DECAY):\n",
    "    \"\"\"\n",
    "    Treina um modelo torchvision com dados da NIH, dados hiperparâmetros de alto nível.\n",
    "\n",
    "    Args:\n",
    "        PATH_TO_IMAGES: caminho para as imagens da NIH\n",
    "        LR: taxa de aprendizado\n",
    "        WEIGHT_DECAY: parâmetro de decaimento de peso para SGD\n",
    "\n",
    "    Returns:\n",
    "        preds: previsões do modelo torchvision no conjunto de teste com a verdadeira para comparação\n",
    "        aucs: AUCs para cada par de treino e teste\n",
    "\n",
    "    \"\"\"\n",
    "    NUM_EPOCHS = 20\n",
    "    BATCH_SIZE = 14\n",
    "\n",
    "    try:\n",
    "        rmtree('results/')\n",
    "    except BaseException:\n",
    "        pass  # o diretório ainda não existe, não há necessidade de limpá-lo\n",
    "    os.makedirs(\"results/\")\n",
    "\n",
    "    # use a média e o desvio padrão do ImageNet para normalização\n",
    "    mean = [0.485, 0.456, 0.406]\n",
    "    std = [0.229, 0.224, 0.225]\n",
    "\n",
    "    N_LABELS = 14  # estamos prevendo 14 rótulos\n",
    "\n",
    "    # defina as transformações torchvision\n",
    "    data_transforms = {\n",
    "        'train': transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.Resize(224),\n",
    "            # porque o redimensionamento nem sempre dá 224 x 224, isso garante 224 x 224\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean, std)\n",
    "        ]),\n",
    "        'val': transforms.Compose([\n",
    "            transforms.Resize(224),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean, std)\n",
    "        ]),\n",
    "    }\n",
    "\n",
    "    # crie dataloaders de treino/val\n",
    "    transformed_datasets = {}\n",
    "    transformed_datasets['train'] = CXR.CXRDataset(\n",
    "        path_to_images=PATH_TO_IMAGES,\n",
    "        fold='train',\n",
    "        transform=data_transforms['train'])\n",
    "    transformed_datasets['val'] = CXR.CXRDataset(\n",
    "        path_to_images=PATH_TO_IMAGES,\n",
    "        fold='val',\n",
    "        transform=data_transforms['val'])\n",
    "\n",
    "    dataloaders = {}\n",
    "    dataloaders['train'] = torch.utils.data.DataLoader(\n",
    "        transformed_datasets['train'],\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=8)\n",
    "    dataloaders['val'] = torch.utils.data.DataLoader(\n",
    "        transformed_datasets['val'],\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=8)\n",
    "    \n",
    "    \n",
    "    if not use_gpu:\n",
    "        raise ValueError(\"Erro, requer GPU\")\n",
    "    \n",
    "    model = models.densenet121(weights='DEFAULT')\n",
    "    num_ftrs = model.classifier.in_features\n",
    "    # adicione a camada final com # de saídas na mesma dimensão dos rótulos com ativação sigmoidal\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Linear(num_ftrs, N_LABELS), nn.Sigmoid())\n",
    "\n",
    "    # coloque o modelo na GPU\n",
    "    model = model.cuda()\n",
    "\n",
    "    # defina o critério, otimizador para treinamento\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.SGD(\n",
    "        filter(\n",
    "            lambda p: p.requires_grad,\n",
    "            model.parameters()),\n",
    "        lr=LR,\n",
    "        momentum=0.9,\n",
    "        weight_decay=WEIGHT_DECAY)\n",
    "    dataset_sizes = {x: len(transformed_datasets[x]) for x in ['train', 'val']}\n",
    "\n",
    "    # treine o modelo\n",
    "    model, best_epoch = train_model(model, criterion, optimizer, LR, num_epochs=NUM_EPOCHS,\n",
    "                                    dataloaders=dataloaders, dataset_sizes=dataset_sizes, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "    # obtenha previsões e AUCs no conjunto de teste\n",
    "    preds, aucs = E.make_pred_multilabel(\n",
    "        data_transforms, model, PATH_TO_IMAGES)\n",
    "\n",
    "    return preds, aucs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-13T22:37:42.228222200Z",
     "start_time": "2023-09-13T22:30:31.432742300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/20\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 72/72 [00:25<00:00,  2.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch 1:loss 0.2166 with data size 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 72/72 [00:18<00:00,  3.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val epoch 1:loss 0.1755 with data size 1000\n",
      "Salvando checkpoint...\n",
      "epoch 2/20\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 72/72 [00:22<00:00,  3.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch 2:loss 0.1651 with data size 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 72/72 [00:18<00:00,  3.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val epoch 2:loss 0.1744 with data size 1000\n",
      "Salvando checkpoint...\n",
      "epoch 3/20\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 72/72 [00:22<00:00,  3.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch 3:loss 0.1545 with data size 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 72/72 [00:17<00:00,  4.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val epoch 3:loss 0.1710 with data size 1000\n",
      "Salvando checkpoint...\n",
      "epoch 4/20\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 72/72 [00:22<00:00,  3.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch 4:loss 0.1436 with data size 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 72/72 [00:18<00:00,  3.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val epoch 4:loss 0.1722 with data size 1000\n",
      "diminuindo a taxa de aprendizado de 0.01 para 0.001 pois não estamos vendo melhoria na perda de validação\n",
      "criado novo otimizador com taxa de aprendizado 0.001\n",
      "epoch 5/20\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 72/72 [00:22<00:00,  3.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch 5:loss 0.1310 with data size 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 72/72 [00:18<00:00,  3.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val epoch 5:loss 0.1703 with data size 1000\n",
      "Salvando checkpoint...\n",
      "epoch 6/20\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 72/72 [00:22<00:00,  3.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch 6:loss 0.1277 with data size 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 72/72 [00:18<00:00,  3.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val epoch 6:loss 0.1712 with data size 1000\n",
      "diminuindo a taxa de aprendizado de 0.001 para 0.0001 pois não estamos vendo melhoria na perda de validação\n",
      "criado novo otimizador com taxa de aprendizado 0.0001\n",
      "epoch 7/20\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 72/72 [00:22<00:00,  3.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch 7:loss 0.1256 with data size 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 72/72 [00:18<00:00,  3.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val epoch 7:loss 0.1698 with data size 1000\n",
      "Salvando checkpoint...\n",
      "epoch 8/20\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 72/72 [00:22<00:00,  3.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch 8:loss 0.1250 with data size 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 72/72 [00:18<00:00,  3.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val epoch 8:loss 0.1703 with data size 1000\n",
      "diminuindo a taxa de aprendizado de 0.0001 para 1e-05 pois não estamos vendo melhoria na perda de validação\n",
      "criado novo otimizador com taxa de aprendizado 1e-05\n",
      "epoch 9/20\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 72/72 [00:22<00:00,  3.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch 9:loss 0.1254 with data size 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 72/72 [00:18<00:00,  3.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val epoch 9:loss 0.1712 with data size 1000\n",
      "diminuindo a taxa de aprendizado de 1e-05 para 1.0000000000000002e-06 pois não estamos vendo melhoria na perda de validação\n",
      "criado novo otimizador com taxa de aprendizado 1.0000000000000002e-06\n",
      "epoch 10/20\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 72/72 [00:22<00:00,  3.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch 10:loss 0.1250 with data size 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 72/72 [00:17<00:00,  4.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val epoch 10:loss 0.1707 with data size 1000\n",
      "diminuindo a taxa de aprendizado de 1.0000000000000002e-06 para 1.0000000000000002e-07 pois não estamos vendo melhoria na perda de validação\n",
      "criado novo otimizador com taxa de aprendizado 1.0000000000000002e-07\n",
      "sem melhoria em 3 epochs, interrompendo\n",
      "Treinamento completo em 6m 50s\n",
      "0\n",
      "160\n",
      "320\n",
      "480\n",
      "640\n",
      "800\n",
      "960\n"
     ]
    }
   ],
   "source": [
    "PATH_TO_IMAGES = \"./images/\"\n",
    "WEIGHT_DECAY = 1e-4\n",
    "LEARNING_RATE = 0.01\n",
    "preds, aucs = train_cnn(PATH_TO_IMAGES, LEARNING_RATE, WEIGHT_DECAY)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
